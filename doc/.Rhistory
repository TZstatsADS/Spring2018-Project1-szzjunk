filter( File%in%inaug.list.after1950.negative$File,Term%in%inaug.list.after1950.negative$Term)%>%  # %in% have to match with vector
select(File,anger:trust)
emo.means=colMeans(select(emotions.after1950.negative, anger:trust)>0.01)
col.use=c("red2", "darkgoldenrod1",
"chartreuse3", "blueviolet",
"darkgoldenrod2", "dodgerblue3",
"darkgoldenrod1", "darkgoldenrod1")
barplot(emo.means[order(emo.means)], las=2, col=col.use[order(emo.means)], horiz=T, main="Inaugural Speeches")
View(dtm)
View(docs)
corpus.list.df
View(corpus.list)
View(corpus.list)
# LDA
corpus.list.positive <- tbl_df(corpus.list)%>%
filter( File%in%inaug.list.after1950.positive$File,Term%in%inaug.list.after1950.positive$Term)  # %in% have to match with vector
View(corpus.list.positive)
docs.positive <- Corpus(VectorSource(corpus.list.positive$snipets))
writeLines(as.character(docs.positive[[sample(1:nrow(corpus.list), 1)]]))
#remove potentially problematic symbols
docs.positive <-tm_map(docs.positive,content_transformer(tolower))
writeLines(as.character(docs.positive[[sample(1:nrow(corpus.list), 1)]]))
docs.positive <- Corpus(VectorSource(corpus.list.positive$snipets))
writeLines(as.character(docs.positive[[sample(1:nrow(corpus.list.positive), 1)]]))
#remove potentially problematic symbols
docs.positive <-tm_map(docs.positive,content_transformer(tolower))
writeLines(as.character(docs.positive[[sample(1:nrow(corpus.list.positive), 1)]]))
#remove punctuation
docs.positive <- tm_map(docs.positive, removePunctuation)
writeLines(as.character(docs.positive[[sample(1:nrow(corpus.list.positive), 1)]]))
#Strip digits
docs.positive <- tm_map(docs.positive, removeNumbers)
docs.positive
#remove stopwords
docs.positive <- tm_map(docs.positive, removeWords, stopwords("english"))
writeLines(as.character(docs.positive[[sample(1:nrow(corpus.list.positive), 1)]]))
#remove whitespace
docs.positive <- tm_map(docs.positive, stripWhitespace)
#Stem document
docs.positive <- tm_map(docs.positive,stemDocument)
writeLines(as.character(docs.positive[[sample(1:nrow(corpus.list.positive), 1)]]))
writeLines(as.character(docs.positive[[sample(1:nrow(corpus.list.positive), 1)]]))
dtm <- DocumentTermMatrix(docs.positive)
dtm.positive <- DocumentTermMatrix(docs.positive)
?DocumentTermMatrix
#convert rownames to filenames#convert rownames to filenames
rownames(dtm.positive) <- paste(corpus.list.positive$type, corpus.list.positive$File,
corpus.list.positive$Term, corpus.list.positive$sent.id, sep="_")
rowTotals <- apply(dtm.positive , 1, sum) #Find the sum of words in each Document
dtm.positive  <- dtm.positive[rowTotals> 0, ]
corpus.list.positive=corpus.list.positive[rowTotals>0, ]
corpus.list.positive <- tbl_df(corpus.list)%>%
filter( File%in%inaug.list.after1950.positive$File,Term%in%inaug.list.after1950.positive$Term,type%in%c("inaug"))  # %in% have to match with vector
docs.positive <- Corpus(VectorSource(corpus.list.positive$snipets))
writeLines(as.character(docs.positive[[sample(1:nrow(corpus.list.positive), 1)]]))
#remove potentially problematic symbols
docs.positive <-tm_map(docs.positive,content_transformer(tolower))
#remove punctuation
docs.positive <- tm_map(docs.positive, removePunctuation)
writeLines(as.character(docs.positive[[sample(1:nrow(corpus.list.positive), 1)]]))
#Strip digits
docs.positive <- tm_map(docs.positive, removeNumbers)
#remove stopwords
docs.positive <- tm_map(docs.positive, removeWords, stopwords("english"))
#remove whitespace
docs.positive <- tm_map(docs.positive, stripWhitespace)
#Stem document
docs.positive <- tm_map(docs.positive,stemDocument)
writeLines(as.character(docs.positive[[sample(1:nrow(corpus.list.positive), 1)]]))
dtm.positive <- DocumentTermMatrix(docs.positive)
?DocumentTermMatrix
#convert rownames to filenames#convert rownames to filenames
rownames(dtm.positive) <- paste(corpus.list.positive$type, corpus.list.positive$File,
corpus.list.positive$Term, corpus.list.positive$sent.id, sep="_")
rowTotals <- apply(dtm.positive , 1, sum) #Find the sum of words in each Document
dtm.positive  <- dtm.positive[rowTotals> 0, ]
corpus.list.positive=corpus.list.positive[rowTotals>0, ]
burnin <- 4000
iter <- 2000
thin <- 500
seed <-list(2003,5,63,100001,765)
nstart <- 5
best <- TRUE
#Number of topics
k <- 15
#Run LDA using Gibbs sampling
ldaOut.positive <-LDA(dtm.positive, k, method="Gibbs", control=list(nstart=nstart,
seed = seed, best=best,
burnin = burnin, iter = iter,
thin=thin))
dtm.positive
#write out results
#docs to topics
ldaOut.topics.positive <- as.matrix(topics(ldaOut.positive))
table(c(1:k, ldaOut.topics.positive))
write.csv(ldaOut.topics.positive,file=paste("../out/LDAGibbs",k,"DocsToTopicsPositive.csv"))
#top 6 terms in each topic
ldaOut.terms.positive <- as.matrix(terms(ldaOut,20))
write.csv(ldaOut.terms.positive,file=paste("../out/LDAGibbs",k,"TopicsToTermsPositive.csv"))
#probabilities associated with each topic assignment
topicProbabilities.positive <- as.data.frame(ldaOut.positive@gamma)
write.csv(topicProbabilities.positive,file=paste("../out/LDAGibbs",k,"TopicProbabilitiesPositive.csv"))
terms.beta.positive=ldaOut.positive@beta
terms.beta.positive=scale(terms.beta)
?scale
topics.terms.positive=NULL
for(i in 1:k){
topics.terms.positive=topics.terms.positive(topics.terms.positive, ldaOut.positive@terms[order(terms.beta.positive[i,], decreasing = TRUE)[1:7]])
}
topics.terms.positive
ldaOut.terms.positive
for(i in 1:k){
topics.terms.positive=topics.terms(topics.terms.positive, ldaOut.positive@terms[order(terms.beta.positive[i,], decreasing = TRUE)[1:7]])
}
for(i in 1:k){
topics.terms.positive=rbind(topics.terms.positive, ldaOut.positive@terms[order(terms.beta.positive[i,], decreasing = TRUE)[1:7]])
}
topics.terms.positive
ldaOut.terms.positive
topics.terms
topics.terms.positive
docs.positive
ldaOut.terms.positive
# LDA
corpus.list.positive <- tbl_df(corpus.list)%>%
filter( File%in%inaug.list.after1950.positive$File,type%in%c("inaug"))  # %in% have to match with vector
View(corpus.list.positive)
View(corpus.list.positive)
docs.positive <- Corpus(VectorSource(corpus.list.positive$snipets))
writeLines(as.character(docs.positive[[sample(1:nrow(corpus.list.positive), 1)]]))
#remove potentially problematic symbols
docs.positive <-tm_map(docs.positive,content_transformer(tolower))
#remove punctuation
docs.positive <- tm_map(docs.positive, removePunctuation)
writeLines(as.character(docs.positive[[sample(1:nrow(corpus.list.positive), 1)]]))
#Strip digits
docs.positive <- tm_map(docs.positive, removeNumbers)
#remove stopwords
docs.positive <- tm_map(docs.positive, removeWords, stopwords("english"))
#remove whitespace
docs.positive <- tm_map(docs.positive, stripWhitespace)
#Stem document
docs.positive <- tm_map(docs.positive,stemDocument)
writeLines(as.character(docs.positive[[sample(1:nrow(corpus.list.positive), 1)]]))
dtm.positive <- DocumentTermMatrix(docs.positive)
#convert rownames to filenames#convert rownames to filenames
rownames(dtm.positive) <- paste(corpus.list.positive$type, corpus.list.positive$File,
corpus.list.positive$Term, corpus.list.positive$sent.id, sep="_")
rowTotals <- apply(dtm.positive , 1, sum) #Find the sum of words in each Document
dtm.positive  <- dtm.positive[rowTotals> 0, ]
corpus.list.positive=corpus.list.positive[rowTotals>0, ]
#Set parameters for Gibbs sampling
burnin <- 4000     # The first burnin iterations are discarded
iter <- 2000
thin <- 500        # every thin iteration is returned for iter iterations
seed <-list(2003,5,63,100001,765)    # needs to have the length nstart
nstart <- 5    #  indicates the number of repeated runs with random initializations
best <- TRUE  # If best=TRUE only the best model over all runs with respect to the log-likelihood is returned
#Number of topics
k <- 15
#Run LDA using Gibbs sampling
ldaOut.positive <-LDA(dtm.positive, k, method="Gibbs", control=list(nstart=nstart,
seed = seed, best=best,
burnin = burnin, iter = iter,
thin=thin))
#write out results
#docs to topics
ldaOut.topics.positive <- as.matrix(topics(ldaOut.positive))
table(c(1:k, ldaOut.topics.positive))
write.csv(ldaOut.topics.positive,file=paste("../out/LDAGibbs",k,"DocsToTopicsPositive.csv"))
#write out results
#docs to topics
ldaOut.topics.positive <- as.matrix(topics(ldaOut.positive))
table(c(1:k, ldaOut.topics.positive))
write.csv(ldaOut.topics.positive,file=paste("../out/LDAGibbs",k,"DocsToTopicsPositive.csv"))
#write out results
#docs to topics
ldaOut.topics.positive <- as.matrix(topics(ldaOut.positive))
table(c(1:k, ldaOut.topics.positive))
write.csv(ldaOut.topics.positive,file=paste("../out/LDAGibbs",k,"DocsToTopicsPositive.csv"))
#top 6 terms in each topic
ldaOut.terms.positive <- as.matrix(terms(ldaOut.positive,20))
write.csv(ldaOut.terms.positive,file=paste("../out/LDAGibbs",k,"TopicsToTermsPositive.csv"))
write.csv(ldaOut.topics.positive,file=paste("../output/LDAGibbs",k,"DocsToTopicsPositive.csv"))
getwd()
setwd(C:/Users/KevinZhang/Documents/GitHub/Spring2018-Project1-szzjunk/doc)
setwd("C:/Users/KevinZhang/Documents/GitHub/Spring2018-Project1-szzjunk/doc")
write.csv(ldaOut.topics.positive,file=paste("../output/LDAGibbs",k,"DocsToTopicsPositive.csv"))
#top 20 terms in each topic
ldaOut.terms.positive <- as.matrix(terms(ldaOut.positive,20))
write.csv(ldaOut.terms.positive,file=paste("../output/LDAGibbs",k,"TopicsToTermsPositive.csv"))
#probabilities associated with each topic assignment
topicProbabilities.positive <- as.data.frame(ldaOut.positive@gamma)
write.csv(topicProbabilities.positive,file=paste("../output/LDAGibbs",k,"TopicProbabilitiesPositive.csv"))
terms.beta.positive=ldaOut.positive@beta
terms.beta.positive=scale(terms.beta.positive)
topics.terms.positive=NULL
for(i in 1:k){
topics.terms.positive=rbind(topics.terms.positive, ldaOut.positive@terms[order(terms.beta.positive[i,], decreasing = TRUE)[1:7]])
}
topics.terms.positive
ldaOut.terms.positive
View(topics.terms.positive)
View(topics.terms.positive)
View(topics.terms.positive)
View(ldaOut.terms.positive)
View(ldaOut.topics.positive)
View(ldaOut.terms.positive)
#write out results
#docs to topics
ldaOut.topics.positive <- as.matrix(topics(ldaOut.positive))
table(c(1:k, ldaOut.topics.positive))
write.csv(ldaOut.topics.positive,file=paste("../output/LDAGibbs",k,"DocsToTopicsPositive.csv"))
#top 6 terms in each topic
ldaOut.terms.positive <- as.matrix(terms(ldaOut.positive,6))
write.csv(ldaOut.terms.positive,file=paste("../output/LDAGibbs",k,"TopicsToTermsPositive.csv"))
#probabilities associated with each topic assignment
topicProbabilities.positive <- as.data.frame(ldaOut.positive@gamma)
write.csv(topicProbabilities.positive,file=paste("../output/LDAGibbs",k,"TopicProbabilitiesPositive.csv"))
terms.beta.positive=ldaOut.positive@beta
terms.beta.positive=scale(terms.beta.positive)
?scale
topics.terms.positive=NULL
for(i in 1:k){
topics.terms.positive=rbind(topics.terms.positive, ldaOut.positive@terms[order(terms.beta.positive[i,], decreasing = TRUE)[1:7]])
}
topics.terms.positive
ldaOut.terms.positive
#write out results
#docs to topics
ldaOut.topics.positive <- as.matrix(topics(ldaOut.positive))
table(c(1:k, ldaOut.topics.positive))
write.csv(ldaOut.topics.positive,file=paste("../output/LDAGibbs",k,"DocsToTopicsPositive.csv"))
#top 15 terms in each topic
ldaOut.terms.positive <- as.matrix(terms(ldaOut.positive,15))
write.csv(ldaOut.terms.positive,file=paste("../output/LDAGibbs",k,"TopicsToTermsPositive.csv"))
#probabilities associated with each topic assignment
topicProbabilities.positive <- as.data.frame(ldaOut.positive@gamma)
write.csv(topicProbabilities.positive,file=paste("../output/LDAGibbs",k,"TopicProbabilitiesPositive.csv"))
terms.beta.positive=ldaOut.positive@beta
terms.beta.positive=scale(terms.beta.positive)
?scale
topics.terms.positive=NULL
for(i in 1:k){
topics.terms.positive=rbind(topics.terms.positive, ldaOut.positive@terms[order(terms.beta.positive[i,], decreasing = TRUE)[1:7]])
}
topics.terms.positive
ldaOut.terms.positive
View(ldaOut.terms.positive)
View(topicProbabilities.positive)
ldaOut.topics.positive <- as.matrix(topics(ldaOut.positive))
table(c(1:k, ldaOut.topics.positive))
write.csv(ldaOut.topics.positive,file=paste("../output/LDAGibbs",k,"DocsToTopicsPositive.csv"))
#top 20 terms in each topic
ldaOut.terms.positive <- as.matrix(terms(ldaOut.positive,20))
write.csv(ldaOut.terms.positive,file=paste("../output/LDAGibbs",k,"TopicsToTermsPositive.csv"))
#probabilities associated with each topic assignment
topicProbabilities.positive <- as.data.frame(ldaOut.positive@gamma)
write.csv(topicProbabilities.positive,file=paste("../output/LDAGibbs",k,"TopicProbabilitiesPositive.csv"))
terms.beta.positive
terms.beta.positive=ldaOut.positive@beta
terms.beta.positive=scale(terms.beta.positive)
terms.beta.positive
topics.terms.positive=NULL
for(i in 1:k){
topics.terms.positive=rbind(topics.terms.positive, ldaOut.positive@terms[order(terms.beta.positive[i,], decreasing = TRUE)])
}
topics.terms.positive
ldaOut.terms.positive
topics.terms.positive=NULL
for(i in 1:k){
topics.terms.positive=rbind(topics.terms.positive, ldaOut.positive@terms[order(terms.beta.positive[i,], decreasing = TRUE)])
}
topics.terms.positive
ldaOut.terms.positive
View(topics.terms.positive)
corpus.list.negative <- tbl_df(corpus.list)%>%
filter( File%in%inaug.list.after1950.negative$File,type%in%c("inaug"))  # %in% have to match with vector
docs.negative <- Corpus(VectorSource(corpus.list.negative$snipets))
writeLines(as.character(docs.negative[[sample(1:nrow(corpus.list.negative), 1)]]))
#remove potentially problematic symbols
docs.negative <-tm_map(docs.negative,content_transformer(tolower))
#remove punctuation
docs.negative <- tm_map(docs.negative, removePunctuation)
writeLines(as.character(docs.negative[[sample(1:nrow(corpus.list.negative), 1)]]))
#Strip digits
docs.negative <- tm_map(docs.negative, removeNumbers)
#remove stopwords
docs.negative <- tm_map(docs.negative, removeWords, stopwords("english"))
#remove whitespace
docs.negative <- tm_map(docs.negative, stripWhitespace)
#Stem document
docs.negative <- tm_map(docs.negative,stemDocument)
writeLines(as.character(docs.negative[[sample(1:nrow(corpus.list.negative), 1)]]))
dtm.negative <- DocumentTermMatrix(docs.negative)
?DocumentTermMatrix
#convert rownames to filenames#convert rownames to filenames
rownames(dtm.negative) <- paste(corpus.list.negative$type, corpus.list.negative$File,
corpus.list.negative$Term, corpus.list.negative$sent.id, sep="_")
rowTotals <- apply(dtm.negative , 1, sum) #Find the sum of words in each Document
dtm.negative  <- dtm.negative[rowTotals> 0, ]
corpus.list.negative=corpus.list.negative[rowTotals>0, ]
#Set parameters for Gibbs sampling
burnin <- 4000     # The first burnin iterations are discarded
iter <- 2000
thin <- 500        # every thin iteration is returned for iter iterations
seed <-list(2003,5,63,100001,765)    # needs to have the length nstart
nstart <- 5    #  indicates the number of repeated runs with random initializations
best <- TRUE  # If best=TRUE only the best model over all runs with respect to the log-likelihood is returned
#Number of topics
k <- 15
#Run LDA using Gibbs sampling
ldaOut.negative <-LDA(dtm.negative, k, method="Gibbs", control=list(nstart=nstart,
seed = seed, best=best,
burnin = burnin, iter = iter,
thin=thin))
#write out results
#docs to topics
ldaOut.topics.negative <- as.matrix(topics(ldaOut.negative))
table(c(1:k, ldaOut.topics.negative))
write.csv(ldaOut.topics.negative,file=paste("../output/LDAGibbs",k,"DocsToTopicsNegative.csv"))
#top 20 terms in each topic
ldaOut.terms.negative <- as.matrix(terms(ldaOut.negative,20))
write.csv(ldaOut.terms.negative,file=paste("../output/LDAGibbs",k,"TopicsToTermsNegative.csv"))
#probabilities associated with each topic assignment
topicProbabilities.negative <- as.data.frame(ldaOut.negative@gamma)
write.csv(topicProbabilities.negative,file=paste("../output/LDAGibbs",k,"TopicProbabilitiesNegative.csv"))
terms.beta.negative=ldaOut.negative@beta
terms.beta.negative=scale(terms.beta.negative)
?scale
topics.terms.negative=NULL
for(i in 1:k){
topics.terms.negative=rbind(topics.terms.negative, ldaOut.negative@terms[order(terms.beta.negative[i,], decreasing = TRUE)[1:7]])
}
topics.terms.negative
ldaOut.terms.negative
shinyApp(
ui = fluidPage(
fluidRow(style = "padding-bottom: 20px;",
column(4, selectInput('speech1', 'Speech 1', speeches, selected=speeches[5])),
column(4, selectInput('speech2', 'Speech 2', speeches, selected=speeches[9])),
column(4, sliderInput('nwords', 'Number of words', 3, min = 20,
max = 200, value=100, step = 20))
),
fluidRow(
plotOutput('wordclouds', height = "400px")
)
),
server = function(input, output, session) {
# Combine the selected variables into a new data frame
selectedData <- reactive({
list(dtm.term1=ff.dtm$term[ff.dtm$document==as.character(which(speeches == input$speech1))],
dtm.count1=ff.dtm$count[ff.dtm$document==as.character(which(speeches == input$speech1))],
dtm.term2=ff.dtm$term[ff.dtm$document==as.character(which(speeches == input$speech2))],
dtm.count2=ff.dtm$count[ff.dtm$document==as.character(which(speeches == input$speech2))])
})
output$wordclouds <- renderPlot(height = 400, {
par(mfrow=c(1,2), mar = c(0, 0, 3, 0))
wordcloud(selectedData()$dtm.term1,
selectedData()$dtm.count1,
scale=c(4,0.5),
max.words=input$nwords,
min.freq=1,
random.order=FALSE,
rot.per=0,
use.r.layout=FALSE,
random.color=FALSE,
colors=brewer.pal(10,"Blues"),
main=input$speech1)
wordcloud(selectedData()$dtm.term2,
selectedData()$dtm.count2,
scale=c(4,0.5),
max.words=input$nwords,
min.freq=1,
random.order=FALSE,
rot.per=0,
use.r.layout=FALSE,
random.color=FALSE,
colors=brewer.pal(10,"Blues"),
main=input$speech2)
})
},
options = list(height = 600)
)
shinyApp(
ui = fluidPage(
fluidRow(style = "padding-bottom: 20px;",
column(4, selectInput('speech1', 'Speech 1', speeches, selected=speeches[5])),
column(4, selectInput('speech2', 'Speech 2', speeches, selected=speeches[9])),
column(4, sliderInput('nwords', 'Number of words', 3, min = 20,
max = 200, value=100, step = 20))
),
fluidRow(
plotOutput('wordclouds', height = "400px")
)
),
server = function(input, output, session) {
# Combine the selected variables into a new data frame
selectedData <- reactive({
list(dtm.term1=ff.dtm$term[ff.dtm$document==as.character(which(speeches == input$speech1))],
dtm.count1=ff.dtm$count[ff.dtm$document==as.character(which(speeches == input$speech1))],
dtm.term2=ff.dtm$term[ff.dtm$document==as.character(which(speeches == input$speech2))],
dtm.count2=ff.dtm$count[ff.dtm$document==as.character(which(speeches == input$speech2))])
})
output$wordclouds <- renderPlot(height = 400, {
par(mfrow=c(1,2), mar = c(0, 0, 3, 0))
wordcloud(selectedData()$dtm.term1,
selectedData()$dtm.count1,
scale=c(4,0.5),
max.words=input$nwords,
min.freq=1,
random.order=FALSE,
rot.per=0,
use.r.layout=FALSE,
random.color=FALSE,
colors=brewer.pal(10,"Blues"),
main=input$speech1)
wordcloud(selectedData()$dtm.term2,
selectedData()$dtm.count2,
scale=c(4,0.5),
max.words=input$nwords,
min.freq=1,
random.order=FALSE,
rot.per=0,
use.r.layout=FALSE,
random.color=FALSE,
colors=brewer.pal(10,"Blues"),
main=input$speech2)
})
},
options = list(height = 600)
)
?terms
?terms
terms(ldaOut.negative,20)
ldaOut.negative
ldaOut.topics.negative
## Clustering of topics
```{r, fig.width=10, fig.height=10}
par(mar=c(1,1,1,1))
topic.summary=tbl_df(corpus.list.df)%>%
filter(type%in%c("nomin", "inaug"), File%in%sel.comparison)%>%
select(File, Economy:Legislation)%>%
group_by(File)%>%
summarise_all(funs(mean))
topic.summary=as.data.frame(topic.summary)
View(topic.summary)
rownames(topic.summary)=topic.summary[,1]
topic.summary[,1]
topic.plot=c(1, 13, 9, 11, 8, 3, 7)
print(topics.hash[topic.plot])
heatmap.2(as.matrix(topic.summary[,topic.plot+1]),
scale = "column", key=F,
col = bluered(100),
cexRow = 0.9, cexCol = 0.9, margins = c(8, 8),
trace = "none", density.info = "none")
```{r, fig.width=6, fig.height=6}
heatmap.2(as.matrix(topic.summary[,topic.plot+1]),
scale = "column", key=F,
col = bluered(100),
cexRow = 0.9, cexCol = 0.9, margins = c(8, 8),
trace = "none", density.info = "none")
par(mfrow=c(5, 1), mar=c(1,1,2,0), bty="n", xaxt="n", yaxt="n")
topic.plot=c(1, 13, 14, 15, 8, 9, 12)
print(topics.hash[topic.plot])
speech.df=tbl_df(corpus.list.df)%>%filter(File=="GeorgeBush", type=="nomin",Term==1)%>%select(sent.id, Economy:Legislation)
speech.df=as.matrix(speech.df)
speech.df[,-1]=f.smooth.topic(x=speech.df[,1], y=speech.df[,-1])
topic.plot=c(1, 13, 14, 15, 8, 9, 12)
print(topics.hash[topic.plot])
speech.df=tbl_df(corpus.list.df)%>%filter(File=="GeorgeBush", type=="nomin",Term==1)%>%select(sent.id, Economy:Legislation)
speech.df=as.matrix(speech.df)
speech.df[,-1]=replace(speech.df[,-1], speech.df[,-1]<1/15, 0.001)
speech.df[,-1]=f.smooth.topic(x=speech.df[,1], y=speech.df[,-1])
plot.stacked(speech.df[,1], speech.df[,topic.plot+1],
xlab="Sentences", ylab="Topic share", main="George Bush, Nomination")
speech.df=tbl_df(corpus.list.df)%>%filter(File=="DonaldJTrump", type=="nomin")%>%select(sent.id, Economy:Legislation)
speech.df=as.matrix(speech.df)
speech.df[,-1]=replace(speech.df[,-1], speech.df[,-1]<1/15, 0.001)
speech.df[,-1]=f.smooth.topic(x=speech.df[,1], y=speech.df[,-1])
plot.stacked(speech.df[,1], speech.df[,topic.plot+1],
xlab="Sentences", ylab="Topic share", main="Donald Trump, Nomination")
par(mfrow=c(5, 1), mar=c(1,1,2,0), bty="n", xaxt="n", yaxt="n")
topic.plot=c(1, 13, 14, 15, 8, 9, 12)
print(topics.hash[topic.plot])
speech.df=tbl_df(corpus.list.df)%>%filter(File=="GeorgeBush", type=="inaug", Term==1)%>%select(sent.id, Economy:Legislation)
speech.df=as.matrix(speech.df)
speech.df[,-1]=replace(speech.df[,-1], speech.df[,-1]<1/15, 0.001)
speech.df[,-1]=f.smooth.topic(x=speech.df[,1], y=speech.df[,-1])
plot.stacked(speech.df[,1], speech.df[,topic.plot+1],
xlab="Sentences", ylab="Topic share", main="George Bush, inaugural Speeches")
speech.df=tbl_df(corpus.list.df)%>%filter(type=="nomin", word.count<20)%>%select(sentences, Economy:Legislation)
as.character(speech.df$sentences[apply(as.data.frame(speech.df[,-1]), 2, which.max)])
names(speech.df)[-1]
presid.summary=tbl_df(corpus.list.df)%>%
filter(type=="inaug", File%in%sel.comparison)%>%
select(File, Economy:Legislation)%>%
group_by(File)%>%
summarise_all(funs(mean))
presid.summary=as.data.frame(presid.summary)
rownames(presid.summary)=as.character((presid.summary[,1]))
km.res=kmeans(scale(presid.summary[,-1]), iter.max=200,
5)
fviz_cluster(km.res,
stand=T, repel= TRUE,
data = presid.summary[,-1],
show.clust.cent=FALSE)
```{r, fig.width=5, fig.height=5}
fviz_cluster(km.res,
stand=T, repel= TRUE,
data = presid.summary[,-1],
show.clust.cent=FALSE)
